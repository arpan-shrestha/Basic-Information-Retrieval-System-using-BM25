{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jMpZErefvlkw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "leQRDEpQJKmf",
        "outputId": "a4050035-7340-430e-d58f-d1eeec6c6dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    docs = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                docs[filename] = preprocess(file.read())\n",
        "    return docs\n",
        "\n",
        "# Compute term frequencies and document frequencies\n",
        "def compute_statistics(docs):\n",
        "    doc_count = len(docs)\n",
        "    term_doc_freq = defaultdict(int)  # Document frequency (DF) for each term\n",
        "    term_freq = defaultdict(lambda: defaultdict(int))  # Term frequency (TF) for each term in each document\n",
        "    doc_length = {}  # Store document lengths for normalization\n",
        "    total_doc_length = 0  # Store the total length of all documents\n",
        "\n",
        "    for doc_id, words in docs.items():\n",
        "        doc_length[doc_id] = len(words)\n",
        "        total_doc_length += len(words)\n",
        "\n",
        "        word_set = set(words)\n",
        "        for word in words:\n",
        "            term_freq[doc_id][word] += 1\n",
        "        for word in word_set:\n",
        "            term_doc_freq[word] += 1\n",
        "\n",
        "    avg_doc_length = total_doc_length / doc_count\n",
        "    return term_freq, term_doc_freq, doc_count, doc_length, avg_doc_length\n",
        "\n",
        "def compute_bm25(query, term_freq, term_doc_freq, doc_count, doc_length, avg_doc_length, k1=1.5, b=0.75):\n",
        "    scores = {}\n",
        "    for doc_id in term_freq:\n",
        "        score = 1\n",
        "        for term in query:\n",
        "            tf = term_freq[doc_id].get(term, 0)  # Term frequency in document\n",
        "            df = term_doc_freq.get(term, 0)      # Document frequency\n",
        "            if df > 0:\n",
        "                idf = math.log((doc_count - df + 0.5) / (df + 0.5) + 1)  # Inverse document frequency (IDF)\n",
        "                term_score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length[doc_id] / avg_doc_length))))\n",
        "                score += term_score\n",
        "        scores[doc_id] = score\n",
        "    return scores\n",
        "\n",
        "def retrieve_documents(folder_path, queries, scaling_factor=0.7):\n",
        "    docs = load_documents(folder_path)\n",
        "    term_freq, term_doc_freq, doc_count, doc_length, avg_doc_length = compute_statistics(docs)\n",
        "\n",
        "    # Open the results file in write mode\n",
        "    with open('result.txt', 'w', encoding='utf-8') as result_file:\n",
        "        for query in queries:\n",
        "            processed_query = preprocess(query)  # Preprocess each query\n",
        "            scores = compute_bm25(processed_query, term_freq, term_doc_freq, doc_count, doc_length, avg_doc_length)\n",
        "            max_score = max(scores.values()) if scores else 1.0  # Avoid division by zero\n",
        "\n",
        "            # Rescale the scores to make the maximum score 0.7\n",
        "            for doc_id in scores:\n",
        "                scores[doc_id] = (scores[doc_id] / max_score) * scaling_factor\n",
        "\n",
        "            # Sort documents by score in descending order\n",
        "            ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "            # Write results for the query to the result file\n",
        "            result_file.write(f\"Query: {' '.join(processed_query)}\\n\")\n",
        "            for doc_id, score in ranked_docs:\n",
        "                result_file.write(f\"Document: {doc_id}, Score: {score:.4f}\\n\")\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "folder_path = '/content/docs'\n",
        "queries = [\n",
        "    \"Who is Alice\",\n",
        "    \"The Mad Hatter's Tea Party\",\n",
        "    \"The Queen of Hearts and the croquet game\",\n",
        "    \"Alice's confrontation with the Queen of Hearts\",\n",
        "    \"ORANGE MARMALADE\",\n",
        "    \"she jumped up on to her feet in a moment:\"\n",
        "\n",
        "]\n",
        "retrieve_documents(folder_path, queries)"
      ],
      "metadata": {
        "id": "E8tX9EZjAIAL"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}